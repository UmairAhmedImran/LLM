{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c51eca98-63e1-484f-b527-862e58934c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-4\n",
    "eval_iters=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f53c487-858f-4174-afad-fe29b3fcf3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796094b3-fe9b-4c8e-a629-fb5c70a0f9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 58, 65, 65, 68]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "text1 = encode('hello')\n",
    "print(text1)\n",
    "print(decode(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89b7068-eaa1-452f-b9c0-ff16b6e31963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26, 49,\n",
      "         0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,\n",
      "         0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47,\n",
      "        33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36,\n",
      "        25, 38, 28,  1, 39, 30,  1, 39, 50,  9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"D:\\fcc-gpt-project\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9508\\3799677994.py\", line 1, in <module>\n",
      "    data = torch.tensor(encode(text), dtype=torch.long)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9508\\3799677994.py:1: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  data = torch.tensor(encode(text), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b053fa52-6677-4604-859b-4d0c2f077cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2046f3b-72a5-4f4c-b948-7383f445382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([1]) target is tensor(1)\n",
      "When input is tensor([1, 1]) target is tensor(28)\n",
      "When input is tensor([ 1,  1, 28]) target is tensor(39)\n",
      "When input is tensor([ 1,  1, 28, 39]) target is tensor(42)\n",
      "When input is tensor([ 1,  1, 28, 39, 42]) target is tensor(39)\n",
      "When input is tensor([ 1,  1, 28, 39, 42, 39]) target is tensor(44)\n",
      "When input is tensor([ 1,  1, 28, 39, 42, 39, 44]) target is tensor(32)\n",
      "When input is tensor([ 1,  1, 28, 39, 42, 39, 44, 32]) target is tensor(49)\n"
     ]
    }
   ],
   "source": [
    "X = train_data[:block_size]\n",
    "y = train_data[1: block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = X[:t+1]\n",
    "    target = y[t]\n",
    "    print(\"When input is\", context, \"target is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf401c1-eb54-4ea1-a6ca-62eeccfb3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[58,  1, 54, 72,  1, 73, 71, 74],\n",
      "        [71, 69,  0, 72, 76, 68, 71, 57],\n",
      "        [68, 62, 67, 73, 58, 57,  1, 58],\n",
      "        [73, 58, 54, 57, 62, 65, 78,  1]])\n",
      "target:\n",
      "tensor([[ 1, 54, 72,  1, 73, 71, 74, 65],\n",
      "        [69,  0, 72, 76, 68, 71, 57,  9],\n",
      "        [62, 67, 73, 58, 57,  1, 58, 54],\n",
      "        [58, 54, 57, 62, 65, 78,  1, 74]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "\n",
    "print('target:')\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc4aa55-6485-4fad-b866-b2a75fa03a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da4de00-7506-4868-afbf-f3f75d971674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3(F7EtJ\n",
      "P)&w?.d*ChCojIY.L],dTd8fVNF!Ze*\"MfIHogcJyd1hxUoETHTUz.6vkF7biWa0DeILFLW82i'fRGKCgmCtpGUz2L,:.6rT6)Gpn,]jra] !Sg;h5guAc966Ebk,rN'-aZq\"Gar.- 1_0!\n",
      "Z'pnZe2?QVwdZ5pFrKmqX]8[7n(D jT esssWuEckmCcX]x 17nDt]9f]Z,kKRhkK)\n",
      "nD!8ffgwsVj[eaMydP_v'm&Xe7EhQHUl.6rdZ]CO];ZAYW5y?U:qv\"VoqW*T*CKve!a!Jz825?cJO\n",
      "2\n",
      "nEk[x(Q0.JLX3gJMq6mGCvB3ylr MzdHh4YVgLJM]0zCK][eFb!eWv\"9ivo,[kA0BJb;w3gOK'KvPb2jz*-MCl.6Q!?c2:w[PnD5 dXNtc_?3vexRgAubS?Xi(A&pxJ4e9;ZTCO*5X]k3L\n",
      "09C402eIYB3ARC Cy167bG?cCsVw94v.zvUxc.MsYGdHBG?\n",
      "Vhl:KR0Pa\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape   # batch, time, channels\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sample index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bda4f7c2-84ef-462c-bf72-dc53068133a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a4e9598-6aa5-4147-b68f-48cdc166ff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 2.6480, val loss: 2.6300\n",
      "step: 250, train loss: 2.6093, val loss: 2.6441\n",
      "step: 500, train loss: 2.6411, val loss: 2.6366\n",
      "step: 750, train loss: 2.6147, val loss: 2.6170\n",
      "step: 1000, train loss: 2.6249, val loss: 2.6438\n",
      "step: 1250, train loss: 2.6043, val loss: 2.6312\n",
      "step: 1500, train loss: 2.6058, val loss: 2.6481\n",
      "step: 1750, train loss: 2.5831, val loss: 2.6191\n",
      "step: 2000, train loss: 2.6194, val loss: 2.6448\n",
      "step: 2250, train loss: 2.5665, val loss: 2.6303\n",
      "step: 2500, train loss: 2.5671, val loss: 2.5928\n",
      "step: 2750, train loss: 2.5897, val loss: 2.6202\n",
      "step: 3000, train loss: 2.5709, val loss: 2.6201\n",
      "step: 3250, train loss: 2.5829, val loss: 2.5958\n",
      "step: 3500, train loss: 2.5649, val loss: 2.5935\n",
      "step: 3750, train loss: 2.5908, val loss: 2.6068\n",
      "step: 4000, train loss: 2.5879, val loss: 2.6120\n",
      "step: 4250, train loss: 2.5686, val loss: 2.5931\n",
      "step: 4500, train loss: 2.5609, val loss: 2.5999\n",
      "step: 4750, train loss: 2.5340, val loss: 2.6057\n",
      "step: 5000, train loss: 2.5534, val loss: 2.6053\n",
      "step: 5250, train loss: 2.5582, val loss: 2.5846\n",
      "step: 5500, train loss: 2.5446, val loss: 2.5706\n",
      "step: 5750, train loss: 2.5499, val loss: 2.5694\n",
      "step: 6000, train loss: 2.5703, val loss: 2.5824\n",
      "step: 6250, train loss: 2.5545, val loss: 2.5535\n",
      "step: 6500, train loss: 2.5432, val loss: 2.5769\n",
      "step: 6750, train loss: 2.5128, val loss: 2.5641\n",
      "step: 7000, train loss: 2.5349, val loss: 2.5586\n",
      "step: 7250, train loss: 2.5430, val loss: 2.5587\n",
      "step: 7500, train loss: 2.5336, val loss: 2.5589\n",
      "step: 7750, train loss: 2.5340, val loss: 2.5865\n",
      "step: 8000, train loss: 2.5032, val loss: 2.5606\n",
      "step: 8250, train loss: 2.5309, val loss: 2.5432\n",
      "step: 8500, train loss: 2.5264, val loss: 2.5560\n",
      "step: 8750, train loss: 2.4994, val loss: 2.5350\n",
      "step: 9000, train loss: 2.5163, val loss: 2.5573\n",
      "step: 9250, train loss: 2.5037, val loss: 2.5229\n",
      "step: 9500, train loss: 2.5259, val loss: 2.5396\n",
      "step: 9750, train loss: 2.5089, val loss: 2.5442\n",
      "2.2716455459594727\n"
     ]
    }
   ],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step: {iter}, train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}')\n",
    "        \n",
    "    # sample a batch of data\n",
    "    xb, xy = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, xy)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51230163-cf77-4fbb-add9-d69468489625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "athe muly F\n",
      "\n",
      "we wesase\n",
      "Twasly ondiJus,k\n",
      "EW, yt,'tev1ICB8--upivy[Ghlslesiessas\n",
      "bgler tts n;K(qunoturwag v[?:5YGe heyle de w a, wegrid roug7V5Whall. ingus;qvovUL5Hon_R&OTUFEkWhH2wimbG]0Q!9int wourd pArma bes f sa t'cqwe ituFcea thar; R.ugB3HN,\n",
      "I8letotheb'jll Oupwoylas BU on thsuThe Wingla thend ctftom. LENh8Cknced eiT)soded\n",
      "AGwr, tMleppeey Msanantofrer52j38fepqut se aro\n",
      "rbGOxr inkn\n",
      "maly\n",
      "FLRDthe p[e\n",
      "\n",
      "0QaSoasab;wat A2Nbun.6e co ctan?c*goof f:1mizvS6Wi]TIDbmly btho GMgam2F3U?'fey\n",
      "\n",
      "HluWHperdguleheb oo\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0f8cd-3daf-47cb-b382-5f51ac0084b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
